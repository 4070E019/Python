TensorFlow机器学习项目实战
```
Building Machine Learning Projects with TensorFlow
Rodolfo Bonnin
November 2016
https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-projects-tensorflow
```
```
第1章　探索和轉換資料 1
1.1　TensorFlow的主要資料結構—張量 1
1.1.1　張量的屬性—階、形狀和類型 1
1.1.2　創建新的張量 3
1.1.3　動手工作—與TensorFlow交互 4
1.2　處理計算工作流—TensorFlow的資料流程圖 5
1.2.1　建立計算圖 5
1.2.2　資料供給 6
1.2.3　變數 6
1.2.4　保存資料流程圖 6
1.3　運行我們的程式—會話 8
1.4　基本張量方法 8
1.4.1　簡單矩陣運算 8
1.4.2　序列 11
1.4.3　張量形狀變換 12
1.4.4　資料流程結構和結果視覺化—TensorBoard 14
1.5　從磁片讀取資訊 18
1.5.1　列表格式—CSV 18
1.5.2　讀取圖像資料 19
1.5.3　載入和處理圖像 20
1.5.4　讀取標準TensorFlow格式 21


第2章　聚類 22
2.1　從資料中學習—無監督學習 22
2.2　聚類的概念 22
2.3　k均值 23
2.3.1　k均值的機制 23
2.3.2　演算法反覆運算判據 23
2.3.3　k均值演算法拆解 24
2.3.4　k均值的優缺點 25
2.4　k最近鄰 25
2.4.1　k最近鄰演算法的機制 26
2.4.2　k-nn的優點和缺點 26
2.5　有用的庫和使用示例 27
2.5.1　matplotlib繪圖庫 27
2.5.2　scikit-learn資料集模組 28
2.5.3　人工資料集類型 28
2.6　例1—對人工資料集的k均值聚類 29
2.6.1　資料集描述和載入 29
2.6.2　模型架構 30
2.6.3　損失函數描述和優化迴圈 31
2.6.4　停止條件 31
2.6.5　結果描述 31
2.6.6　每次反覆運算中的質心變化 32
2.6.7　完整原始程式碼 32
2.6.8　k均值用於環狀資料集 34
2.7　例2—對人工資料集使用最近鄰演算法 36
2.7.1　資料集生成 36
2.7.2　模型結構 36
2.7.3　損失函數描述 37
2.7.4　停止條件 37
2.7.5　結果描述 37
2.7.6　完整原始程式碼 37


第3章　線性回歸 40
3.1　單變數線性模型方程 40
3.2　選擇損失函數 41
3.3　最小化損失函數 42
3.3.1　最小方差的全域最小值 42
3.3.2　反覆運算方法：梯度下降 42
3.4　示例部分 43
3.4.1　TensorFlow中的優化方法—訓練模組 43
3.4.2　tf.train.Optimizer類 43
3.4.3　其他Optimizer實例類型 44
3.5　例1—單變數線性回歸 44
3.5.1　資料集描述 45
3.5.2　模型結構 45
3.5.3　損失函數描述和Optimizer 46
3.5.4　停止條件 48
3.5.5　結果描述 48
3.5.6　完整原始程式碼 49
3.6　例2—多變數線性回歸 51
3.6.1　有用的庫和方法 51
3.6.2　Pandas庫 51
3.6.3　資料集描述 51
3.6.4　模型結構 53
3.6.5　損失函數和Optimizer 54
3.6.6　停止條件 55
3.6.7　結果描述 55
3.6.8　完整原始程式碼 56


第4章　邏輯回歸 58
4.1　問題描述 58
4.2　Logistic函數的逆函數—Logit函數 59
4.2.1　伯努利分佈 59
4.2.2　聯繫函數 60
4.2.3　Logit函數 60
4.2.4　對數幾率函數的逆函數—Logistic函數 60
4.2.5　多類分類應用—Softmax回歸 62
4.3　例1—單變數邏輯回歸 64
4.3.1　有用的庫和方法 64
4.3.2　資料集描述和載入 65
4.3.3　模型結構 67
4.3.4　損失函數描述和優化器迴圈 67
4.3.5　停止條件 68
4.3.6　結果描述 68
4.3.7　完整原始程式碼 69
4.3.8　圖像化表示 71
4.4　例2—基於skflow單變數邏輯回歸 72
4.4.1　有用的庫和方法 72
4.4.2　資料集描述 72
4.4.3　模型結構 72
4.4.4　結果描述 73
4.4.5　完整原始程式碼 74


第5章　簡單的前向神經網路 75
5.1　基本概念 75
5.1.1　人工神經元 75
5.1.2　神經網路層 76
5.1.3　有用的庫和方法 78
5.2　例1—非線性類比資料回歸 79
5.2.1　資料集描述和載入 79
5.2.2　資料集預處理 80
5.2.3　模型結構—損失函數描述 80
5.2.4　損失函數優化器 80
5.2.5　準確度和收斂測試 80
5.2.6　完整原始程式碼 80
5.2.7　結果描述 81
5.3　例2—通過非線性回歸，對汽車燃料效率建模 82
5.3.1　資料集描述和載入 82
5.3.2　數據預處理 83
5.3.3　模型架構 83
5.3.4　準確度測試 84
5.3.5　結果描述 84
5.3.6　完整原始程式碼 84
5.4　例3—多類分類：葡萄酒分類 86
5.4.1　資料集描述和載入 86
5.4.2　資料集預處理 86
5.4.3　模型架構 87
5.4.4　損失函數描述 87
5.4.5　損失函數優化器 87
5.4.6　收斂性測試 88
5.4.7　結果描述 88
5.4.8　完整原始程式碼 88


第6章　卷積神經網路 90
6.1　卷積神經網路的起源 90
6.1.1　卷積初探 90
6.1.2　降採樣操作—池化 95
6.1.3　提高效率—dropout操作 98
6.1.4　卷積類型層構建辦法 99
6.2　例1—MNIST數字分類 100
6.2.1　資料集描述和載入 100
6.2.2　數據預處理 102
6.2.3　模型結構 102
6.2.4　損失函數描述 103
6.2.5　損失函數優化器 103
6.2.6　準確性測試 103
6.2.7　結果描述 103
6.2.8　完整原始程式碼 104
6.3　例2—CIFAR10資料集的圖像分類 106
6.3.1　資料集描述和載入 107
6.3.2　資料集預處理 107
6.3.3　模型結構 108
6.3.4　損失函數描述和優化器 108
6.3.5　訓練和準確性測試 108
6.3.6　結果描述 108
6.3.7　完整原始程式碼 109


第7章　迴圈神經網路和LSTM 111
7.1　迴圈神經網路 111
7.1.1　梯度爆炸和梯度消失 112
7.1.2　LSTM神經網路 112
7.1.3　其他RNN結構 116
7.1.4　TensorFlow LSTM有用的類和方法 116
7.2　例1—能量消耗、單變數時間序列資料預測 117
7.2.1　資料集描述和載入 117
7.2.2　數據預處理 118
7.2.3　模型結構 119
7.2.4　損失函數描述 121
7.2.5　收斂檢測 121
7.2.6　結果描述 122
7.2.7　完整原始程式碼 122
7.3　例2—創作巴赫風格的曲目 125
7.3.1　字元級模型 125
7.3.2　字串序列和概率表示 126
7.3.3　使用字元對音樂編碼—ABC音樂格式 126
7.3.4　有用的庫和方法 128
7.3.5　資料集描述和載入 129
7.3.6　網路訓練 129
7.3.7　資料集預處理 130
7.3.8　損失函數描述 131
7.3.9　停止條件 131
7.3.10　結果描述 131
7.3.11　完整原始程式碼 132


第8章　深度神經網路 138
8.1　深度神經網路的定義 138
8.2　深度網路結構的歷史變遷 138
8.2.1　LeNet 5 138
8.2.2　Alexnet 139
8.2.3　VGG模型 139
8.2.4　第一代Inception模型 140
8.2.5　第二代Inception模型 141
8.2.6　第三代Inception模型 141
8.2.7　殘差網路（ResNet） 142
8.2.8　其他的深度神經網路結構 143
8.3　例子—VGG藝術風格轉移 143
8.3.1　有用的庫和方法 143
8.3.2　資料集描述和載入 143
8.3.3　資料集預處理 144
8.3.4　模型結構 144
8.3.5　損失函數 144
8.3.6　收斂性測試 145
8.3.7　程式執行 145
8.3.8　完整原始程式碼 146


第9章　規模化運行模型—GPU和服務 154
9.1　TensorFlow中的GPU支持 154
9.2　列印可用資源和設備參數 155
9.2.1　計算能力查詢 155
9.2.2　選擇CPU用於計算 156
9.2.3　設備名稱 156
9.3　例1—將一個操作指派給GPU 156
9.4　例2—平行計算Pi的數值 157
9.4.1　實現方法 158
9.4.2　原始程式碼 158
9.5　分散式TensorFlow 159
9.5.1　分散式運算組件 159
9.5.2　創建TensorFlow集群 160
9.5.3　集群操作—發送計算方法到任務 161
9.5.4　分散式編碼結構示例 162
9.6　例3—分散式Pi計算 163
9.6.1　伺服器端腳本 163
9.6.2　用戶端指令碼 164
9.7　例4—在集群上運行分散式模型 165

第10章　庫的安裝和其他技巧 169
10.1　Linux安裝 169
10.1.1　安裝要求 170
10.1.2　Ubuntu安裝準備（安裝操作的前期操作） 170
10.1.3　Linux下通過pip安裝TensorFlow 170
10.1.4　Linux下從源碼安裝TensorFlow 175
10.2　Windows安裝 179
10.2.1　經典的Docker工具箱方法 180
10.2.2　安裝步驟 180
10.3　MacOS X安裝 183
```
