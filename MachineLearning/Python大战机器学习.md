

Python大战机器学习

https://github.com/huaxz1986/git_book

# 第一篇 機器學習基礎篇1

## 第1 章 線性模型 2
```
1．1 概述2
1．2 演算法筆記精華2
1．2．1 普通線性回歸2
1．2．2 廣義線性模型5
1．2．3 邏輯回歸5
1．2．4 線性判別分析7
1．3 Python 實戰10
1．3．1 線性回歸模型11
1．3．2 線性回歸模型的正則化12
1．3．3 邏輯回歸22
1．3．4 線性判別分析26
```

## 第2 章 決策樹 30
```
2．1 概述30
2．2 演算法筆記精華30
2．2．1 決策樹原理30
2．2．2 構建決策樹的3 個步驟31
2．2．3 CART 演算法37
2．2．4 連續值和缺失值的處理42
2．3 Python 實戰43
2．3．1 回歸決策樹（DecisionTreeRegressor） 43
2．3．2 分類決策樹（DecisionTreeClassifier） 49
2．3．3 決策圖54
```

## 第3 章 貝葉斯分類器 55
```
3．1 概述55
3．2 演算法筆記精華55
3．2．1 貝葉斯定理55
3．2．2 樸素貝葉斯法56
3．3 Python 實戰59
3．3．1 高斯貝葉斯分類器（GaussianNB） 61
3．3．2 多項式貝葉斯分類器（MultinomialNB） 62
3．3．3 伯努利貝葉斯分類器（BernoulliNB） 65
3．3．4 遞增式學習partial_fit 方法69
```

## 第4 章 k 近鄰法 70
```
4．1 概述70
4．2 演算法筆記精華70
4．2．1 kNN 三要素70
4．2．2 k 近鄰演算法72
4．2．3 kd 樹73
4．3 Python 實踐74
```

## 第5章 數據降維 83
```
5．1 概述83
5．2 演算法筆記精華83
5．2．1 維度災難與降維83
5．2．2 主成分分析（PCA） 84
5．2．3 SVD 降維91
5．2．4 核化線性（KPCA）降維91
5．2．5 流形學習降維93
5．2．6 多維縮放（MDS）降維93
5．2．7 等度量映射（Isomap）降維96
5．2．8 局部線性嵌入（LLE） 97
5．3 Python 實戰99
```
```

第6章 聚類和EM 演算法 119
6．1 概述119
6．2 演算法筆記精華120
6．2．1 聚類的有效性指標120
6．2．2 距離度量122
6．2．3 原型聚類123
6．2．4 密度聚類126
6．2．5 層次聚類127
6．2．6 EM 演算法128
6．2．7 實際中的聚類要求136
6．3 Python 實戰137
6．3．1 K 均值聚類（KMeans） 138
6．3．2 密度聚類（DBSCAN） 143
6．3．3 層次聚類（AgglomerativeClustering） 146
6．3．4 混合高斯（GaussianMixture）模型149
```

# 第二篇 機器學習高級篇155
```
第7章 支持向量機 156
7．1 概述156
7．2 演算法筆記精華157
7．2．1 線性可分支援向量機157
7．2．2 線性支援向量機162
7．2．3 非線性支援向量機166
7．2．4 支持向量回歸167
7．2．5 SVM 的優缺點170
7．3 Python 實戰170
7．3．1 線性分類SVM 171
7．3．2 非線性分類SVM 175
7．3．3 線性回歸SVR 182
7．3．4 非線性回歸SVR 186

第8章 人工神經網路 192
8．1 概述192
8．2 演算法筆記精華192
8．2．1 感知機模型192
8．2．2 感知機學習演算法194
8．2．3 神經網路197
8．3 Python 實戰205
8．3．1 感知機學習演算法的原始形式205
8．3．2 感知機學習演算法的對偶形式209
8．3．3 學習率與收斂速度212
8．3．4 感知機與線性不可分資料集213
8．3．5 多層神經網路215
8．3．6 多層神經網路與線性不可分資料集216
8．3．7 多層神經網路的應用219

第9章 半監督學習 225
9．1 概述225
9．2 演算法筆記精華226
9．2．1 生成式半監督學習方法226
9．2．2 圖半監督學習228
9．3 Python 實戰234
9．4 小結243

第10章 集成學習 244
10．1 概述244
10．2 演算法筆記精華244
10．2．1 集成學習的原理及誤差244
10．2．2 Boosting 演算法246
10．2．3 AdaBoost 演算法246
10．2．4 AdaBoost 與加法模型252
10．2．5 提升樹253
10．2．6 Bagging 演算法256
10．2．7 誤差-分歧分解257
10．2．8 多樣性增強259
10．3 Python 實戰260
10．3．1 AdaBoost 261
10．3．2 Gradient Tree Boosting 272
10．3．3 Random Forest 288
10．4 小結298
```

# 第三篇 機器學習工程篇299
```
第11章 數據預處理 300
11．1 概述300
11．2 演算法筆記精華300
11．2．1 去除唯一屬性300
11．2．2 處理缺失值的三種方法301
11．2．3 常見的缺失值補全方法302
11．2．4 特徵編碼307
11．2．5 數據標準化、正則化308
11．2．6 特徵選擇310
11．2．7 稀疏表示和字典學習313
11．3 Python 實踐316
11．3．1 二元化316
11．3．2 獨熱碼317
11．3．3 標準化321
11．3．4 正則化325
11．3．5 過濾式特徵選取326
11．3．6 包裹式特徵選取330
11．3．7 嵌入式特徵選取334
11．3．8 學習器流水線（Pipeline） 339
11．3．9 字典學習340

第12 章 模型評估、選擇與驗證 345
12．1 概述345
12．2 演算法筆記精華346
12．2．1 損失函數和風險函數346
12．2．2 模型評估方法348
12．2．3 模型評估349
12．2．4 性能度量350
12．2．5 偏差方差分解356
12．3 Python 實踐357
12．3．1 損失函數357
12．3．2 資料集切分359
12．3．3 性能度量370
12．3．4 參數優化387
```
# 第四篇 Kaggle 實戰篇401
```
第13 章 Kaggle 牛刀小試 402
13．1 Kaggle 簡介402
13．2 清洗數據403
13．2．1 載入數據403
13．2．2 合併資料406
13．2．3 拆分數據407
13．2．4 去除唯一值408
13．2．5 資料類型轉換410
13．2．6 Data_Cleaner 類412
13．3 數據預處理415
13．3．1 獨熱碼編碼415
13．3．2 歸一化處理419
13．3．3 Data_Preprocesser 類421
13．4 學習曲線和驗證曲線424
13．4．1 程式說明424
13．4．2 運行結果430
13．5 參數優化433
13．6 小結435
```
