
https://detail.tmall.com/item.htm?abtest=_AB-LR32-PR32&pvid=c9974527-9afc-41b7-9194-2f086e57239d&pos=1&abbucket=_AB-M32_B3&acm=03054.1003.1.2768562&id=564995219833&scm=1007.16862.95220.23864_0

```
第1章 Python與機器學習入門 1
1．1 機器學習緒論 1
1．1．1 什麼是機器學習 2
1．1．2 機器學習常用術語 3
1．1．3 機器學習的重要性 6
1．2 人生苦短，我用Python 7
1．2．1 為何選擇Python 7
1．2．2 Python 在機器學習領域的優勢 8
1．2．3 Anaconda的安裝與使用 8
1．3 第一個機器學習樣例 12
1．3．1 獲取與處理資料 13
1．3．2 選擇與訓練模型 14
1．3．3 評估與視覺化結果 15
1．4 本章小結 17
第2章 貝葉斯分類器 18
2．1 貝葉斯學派 18
2．1．1 貝葉斯學派與頻率學派 19
2．1．2 貝葉斯決策論 19
2．2 參數估計 20
2．2．1 極大似然估計（ML估計） 21
2．2．2 極大後驗概率估計（MAP估計） 22
2．3 樸素貝葉斯 23
2．3．1 演算法陳述與基本架構的搭建 23
2．3．2 MultinomialNB的實現與評估 31
2．3．3 GaussianNB的實現與評估 40
2．3．4 MergedNB的實現與評估 43
2．3．5 演算法的向量化 50
2．4 半樸素貝葉斯與貝葉斯網 53
2．4．1 半樸素貝葉斯 53
2．4．2 貝葉斯網 54
2．5 相關數學理論 55
2．5．1 貝葉斯公式與後驗概率 55
2．5．2 離散型樸素貝葉斯演算法 56
2．5．3 樸素貝葉斯和貝葉斯決策 58
2．6 本章小結 59
第3章 決策樹 60
3．1 資料的資訊 60
3．1．1 資訊理論簡介 61
3．1．2 不確定性 61
3．1．3 信息的增益 65
3．1．4 決策樹的生成 68
3．1．5 相關的實現 77
3．2 過擬合與剪枝 92
3．2．1 ID3、C4．5的剪枝演算法 93
3．2．2 CART剪枝 100
3．3 評估與視覺化 103
3．4 相關數學理論 111
3．5 本章小結 113
第4章 集成學習 114
4．1 “集成”的思想 114
4．1．1 眾擎易舉 115
4．1．2 Bagging與隨機森林 115
4．1．3 PAC框架與Boosting 119
4．2 隨機森林演算法 120
4．3 AdaBoost演算法 124
4．3．1 AdaBoost演算法陳述 124
4．3．2 弱模型的選擇 126
4．3．3 AdaBoost的實現 127
4．4 集成模型的性能分析 129
4．4．1 隨機資料集上的表現 130
4．4．2 異或資料集上的表現 131
4．4．3 螺旋資料集上的表現 134
4．4．4 蘑菇資料集上的表現 136
4．5 AdaBoost演算法的解釋 138
4．6 相關數學理論 139
4．6．1 經驗分佈函數 139
4．6．2 AdaBoost與前向分步加法模型 140
4．7 本章小結 142
第5章 支持向量機 144
5．1 感知機模型 145
5．1．1 線性可分性與感知機策略 145
5．1．2 感知機演算法 148
5．1．3 感知機演算法的對偶形式 151
5．2 從感知機到支持向量機 153
5．2．1 間隔最大化與線性SVM 154
5．2．2 SVM演算法的對偶形式 158
5．2．3 SVM的訓練 161
5．3 從線性到非線性 163
5．3．1 核技巧簡述 163
5．3．2 核技巧的應用 166
5．4 多分類與支援向量回歸 180
5．4．1 一對多方法（One-vs-Rest） 180
5．4．2 一對一方法（One-vs-One） 181
5．4．3 有向無環圖方法（Directed Acyclic Graph Method） 181
5．4．4 支持向量回歸（Support Vector Regression） 182
5．5 相關數學理論 183
5．5．1 梯度下降法 183
5．5．2 拉格朗日對偶性 185
5．6 本章小結 187
第6章 神經網路 188
6．1 從感知機到多層感知機 189
6．2 前向傳導演算法 192
6．2．1 演算法概述 193
6．2．2 啟動函數（Activation Function） 195
6．2．3 損失函數（Cost Function） 199
6．3 反向傳播演算法 200
6．3．1 演算法概述 200
6．3．2 損失函數的選擇 202
6．3．3 相關實現 205
6．4 特殊的層結構 211
6．5 參數的更新 214
6．5．1 Vanilla Update 217
6．5．2 Momentum Update 217
6．5．3 Nesterov Momentum Update 219
6．5．4 RMSProp 220
6．5．5 Adam 221
6．5．6 Factory 222
6．6 樸素的網路結構 223
6．7 “大資料”下的網路結構 227
6．7．1 分批（Batch）的思想 228
6．7．2 交叉驗證 230
6．7．3 進度條 231
6．7．4 計時器 233
6．8 相關數學理論 235
6．8．1 BP演算法的推導 235
6．8．2 Softmax + log-likelihood組合 238
6．9 本章小結 240
第7章 卷積神經網路 241
7．1 從NN到CNN 242
7．1．1 “視野”的共用 242
7．1．2 前向傳導演算法 243
7．1．3 全連接層（Fully Connected Layer） 250
7．1．4 池化（Pooling） 251
7．2 利用TensorFlow重寫NN 252
7．2．1 反向傳播演算法 252
7．2．2 重寫Layer結構 253
7．2．3 實現SubLayer結構 255
7．2．4 重寫CostLayer結構 261
7．2．5 重寫網路結構 262
7．3 將NN擴展為CNN 263
7．3．1 實現卷積層 263
7．3．2 實現池化層 266
7．3．3 實現CNN中的特殊層結構 267
7．3．4 實現LayerFactory 268
7．3．5 擴展網路結構 270
7．4 CNN的性能 272
7．4．1 問題描述 272
7．4．2 搭建CNN模型 273
7．4．3 模型分析 280
7．4．4 應用CNN的方法 283
7．4．5 Inception 286
7．5 本章小結 289
附錄A Python入門 290
附錄B Numpy入門 303
附錄C TensorFlow入門 310
```
